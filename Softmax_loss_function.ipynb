{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, naive implementation (with loops)\n",
    "  Inputs:\n",
    "  - W: C x D array of weights\n",
    "  - X: D x N array of data. Data are D-dimensional columns\n",
    "  - y: 1-dimensional array of length N with labels 0...K-1, for K classes\n",
    "  - reg: (float) regularization strength\n",
    "  Returns:\n",
    "  a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W, an array of same size as W\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # Compute the softmax loss and its gradient using explicit loops.           #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "\n",
    "  # Get shapes\n",
    "  num_classes = W.shape[0]\n",
    "  num_train = X.shape[1]\n",
    "\n",
    "  for i in range(num_train):\n",
    "    # Compute vector of scores\n",
    "    f_i = W.dot(X[:, i]) # in R^{num_classes}\n",
    "\n",
    "    # Normalization trick to avoid numerical instability, per http://cs231n.github.io/linear-classify/#softmax\n",
    "    log_c = np.max(f_i)\n",
    "    f_i -= log_c\n",
    "\n",
    "    # Compute loss (and add to it, divided later)\n",
    "    # L_i = - f(x_i)_{y_i} + log \\sum_j e^{f(x_i)_j}\n",
    "    sum_i = 0.0\n",
    "    for f_i_j in f_i:\n",
    "      sum_i += np.exp(f_i_j)\n",
    "    loss += -f_i[y[i]] + np.log(sum_i)\n",
    "\n",
    "    # Compute gradient\n",
    "    # dw_j = 1/num_train * \\sum_i[x_i * (p(y_i = j)-Ind{y_i = j} )]\n",
    "    # Here we are computing the contribution to the inner sum for a given i.\n",
    "    for j in range(num_classes):\n",
    "      p = np.exp(f_i[j])/sum_i\n",
    "      dW[j, :] += (p-(j == y[i])) * X[:, i]\n",
    "\n",
    "  # Compute average\n",
    "  loss /= num_train\n",
    "  dW /= num_train\n",
    "\n",
    "  # Regularization\n",
    "  loss += 0.5 * reg * np.sum(W * W)\n",
    "  dW += reg*W\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    # SVM loss function native version\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)    # initialize the gradient as zero\n",
    " \n",
    "    # compute the loss and the gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    # For each sample, accumulate loss\n",
    "    for i in xrange(num_train):\n",
    "        scores = X[i].dot(W)     # (1, C)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        for j in xrange(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            # Calculate according to the SVM loss function\n",
    "            margin = scores[j] - correct_class_score + 1    # note delta = 1\n",
    "            # When margin>0, there will be loss, and there will also be accumulation of gradients\n",
    "            if margin > 0:      # max(0, yi - yc + 1)\n",
    "                loss += margin\n",
    "                 # According to the formula: ∇Wyi Li =-xiT(∑j≠yi1(xiWj-xiWyi +1>0)) + 2λWyi\n",
    "                dW[:, y[i]] += -X[i, :]   # y[i] is the correct class\n",
    "                # According to the formula: ∇Wj Li = xiT 1(xiWj-xiWyi +1>0) + 2λWj,\n",
    "                dW[:, j] += X[i, :]\n",
    " \n",
    "    # Average loss of training data\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    " \n",
    "    # Regular loss\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    " \n",
    "    #\n",
    "    return loss, dW\n",
    " \n",
    " \n",
    "#\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "         SVM loss function vectorized version\n",
    "    Structured SVM loss function, vectorized implementation.Inputs and outputs\n",
    "    are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape)   # initialize the gradient as zero\n",
    "    scores = X.dot(W)        # N by C Number of samples*Number of categories\n",
    "    num_train = X.shape[0]\n",
    "    num_classes = W.shape[1]\n",
    " \n",
    "    scores_correct = scores[np.arange(num_train), y]\n",
    "    scores_correct = np.reshape(scores_correct, (num_train, 1))  # N*1 Correct category for each sample\n",
    " \n",
    "    margins = scores - scores_correct + 1.0     # N by C Calculate the loss of each place in the scores matrix\n",
    "    margins[np.arange(num_train), y] = 0.0      # The correct category loss of each sample is set to 0\n",
    "    margins[margins <= 0] = 0.0                 # max(0, x)\n",
    "    loss += np.sum(margins) / num_train         # Accumulate all losses and take the average\n",
    "    loss += 0.5 * reg * np.sum(W * W)           # Regular\n",
    " \n",
    "    # compute the gradient\n",
    "    margins[margins > 0] = 1.0                  # max(0, x) The gradient greater than 0 is counted as 1\n",
    "    row_sum = np.sum(margins, axis=1)           # N*1 Each sample is accumulated\n",
    "    margins[np.arange(num_train), y] = -row_sum  # Class correct position = -gradient accumulation\n",
    "    dW += np.dot(X.T, margins)/num_train + reg * W     # D by C\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0902af6d8f8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msv_softmax_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "\n",
    "def sv_softmax_loss(t=1.0, s=1):\n",
    "\n",
    "    t = float(t)\n",
    "    s = float(s)\n",
    "    \n",
    "    def sv_softmax_loss_fixed(y_true, logits):\n",
    "        \"\"\"SV-Softmax loss\n",
    "        Notice: y_pred is raw logits\n",
    "        Support Vector Guided Softmax Loss for Face Recognition\n",
    "        https://arxiv.org/pdf/1812.11317.pdf\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "        Keyword Arguments:\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "        ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "        \n",
    "        logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "        I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "        \n",
    "        h = tf.exp(s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "        \n",
    "        softmax = tf.exp(s * logits) / (tf.reshape(\n",
    "                         tf.reduce_sum(tf.multiply(tf.exp(s * logits), h), axis=-1, keepdims=True), \n",
    "                         [-1, 1]) + epsilon)\n",
    "        \n",
    "        # We add epsilon because log(0) = nan\n",
    "        softmax = tf.add(softmax, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.log(softmax))\n",
    "        ce = tf.reduce_sum(ce, axis=1)\n",
    "        return tf.reduce_mean(ce)\n",
    "    \n",
    "    return sv_softmax_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
